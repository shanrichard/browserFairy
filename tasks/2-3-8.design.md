# Design 2-3-8: 网络请求调用栈关联

## Requirements

### 核心需求

**目标**：扩展现有NetworkMonitor，收集完整的JavaScript调用栈信息，帮助定位大数据请求的发起代码位置。

**诊断价值**：
- 定位"某组件初始化时意外触发5.2MB数据下载"的具体代码路径
- 发现无用或重复API调用的代码位置  
- 提供原始调用栈数据（非分析指标），供后续针对性分析使用

### 具体场景

**问题场景**：Web应用在某些操作后出现性能问题，怀疑是大数据请求导致
**现有限制**：NetworkMonitor只提供第一层调用信息，无法追踪完整调用链路
**期望结果**：能够看到完整的JavaScript调用栈，从根本发起者到最终网络调用的完整路径

### 非功能性需求

- **性能要求**：基于实际数据分析，仅对~2.7%的URL收集详细栈，不影响97%+的正常请求
- **兼容性要求**：向后兼容现有network.jsonl数据格式
- **可靠性要求**：调用栈收集失败时优雅降级，不影响基础网络监控
- **精准度要求**：能够捕获关键性能问题（如3939次API调用、12.9MB JS文件）

## Solution

### 现有代码分析

**当前NetworkMonitor实现状况**：
```python
# browserfairy/monitors/network.py 当前实现
def _format_initiator_simple(self, initiator: dict) -> dict:
    """Format request initiator information (simplified version)."""
    result = {"type": initiator.get("type", "unknown")}
    
    if initiator.get("stack") and initiator["stack"].get("callFrames"):
        frame = initiator["stack"]["callFrames"][0]  # 只取第一帧
        result["source"] = {
            "function": frame.get("functionName", "anonymous")[:100],
            "url": frame.get("url", "")[:200],
            "line": frame.get("lineNumber", 0)
        }
    
    return result
```

**现有数据格式（network.jsonl）**：
```json
{
  "type": "network_request_complete",
  "url": "https://api.example.com/data.json",
  "initiator": {
    "type": "script",
    "source": {
      "function": "fetchData",
      "url": "https://app.js",
      "line": 42
    }
  }
}
```

**分析结果**：
- ✅ 已有基础调用栈处理框架
- ❌ 只提取第一个调用帧（`callFrames[0]`）
- ❌ 未启用Debugger domain获取详细栈信息
- ❌ 未处理异步调用栈（`parent`属性）
- ❌ 缺少完整调用链路追踪

### 技术设计

#### 1. 最小侵入式扩展方案（v1 取舍）

**设计原则**：
- 保持现有`_format_initiator_simple()`完全不变
- 新增功能仅对大请求启用，不影响常规请求性能
- 向后兼容现有数据格式

**实现策略（两阶段：候选缓存 + 完成确认）**：
```python
class NetworkMonitor:
    def __init__(self, ...):
        # 现有属性保持不变
        self.pending_requests: dict = {}
        self.hostname = None
        
        # 新增栈增强属性
        self.debugger_enabled = False
        # LRU 候选缓存：requestId -> { snapshot, cached_at, url, resource_type }
        self.stack_candidates = {}
        self.max_candidates = 300       # LRU 上限控制内存
        # 计数键均使用 (origin, normalized_path)
        self.api_count = {}             # (origin, path) -> count
        self.resource_count = {}        # (origin, path) -> count
        
    async def start_monitoring(self) -> None:
        """启动监控时全局启用Debugger"""
        # 现有CDP domain启用
        self.connector.on_event("Network.requestWillBeSent", self._on_request_start)
        # ...其他事件监听
        
        # 新增：综合模式全局启用Debugger
        await self._enable_debugger_globally()
        
    async def _enable_debugger_globally(self):
        """全局一次性启用Debugger domain"""
        try:
            await self.connector.call("Debugger.enable", session_id=self.session_id)
            await self.connector.call("Debugger.setAsyncCallStackDepth", 
                                    {"maxDepth": 8}, session_id=self.session_id)
            self.debugger_enabled = True
            logger.debug(f"Debugger enabled for detailed stack collection")
        except Exception as e:
            logger.debug(f"Failed to enable debugger: {e}")
            # 优雅降级，不影响基础网络监控

    async def _on_request_start(self, params: dict) -> None:
        # 现有逻辑完全保持不变
        request_data = {
            "initiator": self._format_initiator_simple(params["initiator"]),
            # ... 所有现有字段
        }
        
        # 新增：判断并缓存候选 initiator（XHR/Fetch 一律缓存；Script 仅当存在 JS 栈）
        candidate_reason = self._should_cache_initiator(params)
        if candidate_reason:
            self._cache_trimmed_initiator(
                request_id=params["requestId"],
                raw_initiator=params["initiator"],
                url=params.get("request", {}).get("url", ""),
                resource_type=params.get("type", "")
            )
            
        # 新增：更新计数（用于后续触发判断）
        self._update_request_counts(params)
        
        # 现有逻辑继续（完全不变）
        self.pending_requests[request_id] = request_data
        self.event_queue.put_nowait(("network_request_start", request_data))
        
    async def _on_request_finished(self, params: dict) -> None:
        # 现有逻辑保持不变...
        request_data = self.pending_requests.pop(request_id)
        
        # 新增：最终确认并附加detailedStack
        final_reason = self._confirm_detailed_stack_needed(url=request_data.get("url", ""), params=params)
        if final_reason:
            candidate = self.stack_candidates.get(request_id)
            if candidate:
                detailed_stack = self._format_detailed_stack(candidate["snapshot"])  # 解析缓存快照
                request_data["detailedStack"] = {
                    "enabled": True,
                    "reason": final_reason,
                    "collectionTime": datetime.now(timezone.utc).isoformat(),
                    **detailed_stack
                }
            else:
                # 无候选：也要落下原因，避免空档
                request_data["detailedStack"] = {
                    "enabled": False,
                    "reason": final_reason,
                    "collectionTime": datetime.now(timezone.utc).isoformat()
                }
            
        # 清理候选缓存
        self.stack_candidates.pop(request_id, None)
        
        # 现有逻辑继续（完全不变）
        self.event_queue.put_nowait(("network_request_complete", request_data))
```

#### 2. 调用栈增强策略

**分层实现**：
1. **基础层**：保持现有简单模式（所有请求）
2. **增强层**：完整栈收集（仅大请求 >1MB）

**内存控制策略（LRU + 裁剪 + 不依赖 pending_requests）**：
```python
def _cache_trimmed_initiator(self, request_id: str, raw_initiator: dict, url: str, resource_type: str):
    """缓存裁剪后的initiator快照"""
    # LRU淘汰策略
    if len(self.stack_candidates) >= self.max_candidates:
        # 基于 cached_at 淘汰最早的候选，避免依赖 pending_requests
        oldest_id = min(self.stack_candidates, key=lambda k: self.stack_candidates[k]["cached_at"])
        self.stack_candidates.pop(oldest_id, None)
    
    # 立即裁剪：frames≤20, asyncFrames≤10, 字符串截断
    trimmed = self._trim_initiator_snapshot(raw_initiator)
    self.stack_candidates[request_id] = {
        "snapshot": trimmed,
        "cached_at": datetime.now(timezone.utc).timestamp(),
        "url": url,
        "resource_type": resource_type
    }

def _trim_initiator_snapshot(self, raw_initiator: dict) -> dict:
    """裁剪initiator快照以控制内存"""
    if not raw_initiator.get("stack"):
        return {"type": raw_initiator.get("type", "unknown")}
        
    trimmed = {"type": raw_initiator.get("type", "unknown")}
    stack = raw_initiator["stack"]
    
    # 裁剪主调用栈
    if stack.get("callFrames"):
        def trim_frames(frames, limit):
            out = []
            for frame in frames[:limit]:
                out.append({
                    "functionName": str(frame.get("functionName", ""))[:100],
                    "url": str(frame.get("url", ""))[:200], 
                    "lineNumber": int(frame.get("lineNumber", 0)),
                    "columnNumber": int(frame.get("columnNumber", 0)),
                    "scriptId": str(frame.get("scriptId", ""))[:50]
                })
            return out

        trimmed_stack = {"callFrames": trim_frames(stack.get("callFrames", []), 20)}

        # 裁剪并重建异步父栈链（最多10层），每层保留裁剪后的 callFrames 与下一级 parent
        parent_src = stack
        parent_dst = trimmed_stack
        depth = 0
        while parent_src.get("parent") and depth < 10:
            parent_src = parent_src["parent"]
            node = {"callFrames": trim_frames(parent_src.get("callFrames", []), 20)}
            parent_dst["parent"] = node
            parent_dst = node
            depth += 1

        trimmed["stack"] = trimmed_stack
    
    return trimmed
```

**完整栈解析算法**：
```python
def _format_detailed_stack(self, trimmed_initiator: dict) -> dict:
    """解析缓存的调用栈快照"""
    frames = []
    async_frames = []
    
    stack = trimmed_initiator.get("stack", {})
    
    # 解析主调用栈
    if stack.get("callFrames"):
        frames = stack["callFrames"]  # 已经在缓存时裁剪过
        
    # 解析异步调用栈（parent属性）
    current_stack = stack
    while current_stack.get("parent"):
        current_stack = current_stack["parent"]
        if current_stack.get("callFrames"):
            async_frames.extend(current_stack["callFrames"])
    
    return {
        "frames": frames,
        "asyncFrames": async_frames[:10],  # 二次保护
        "truncated": len(frames) >= 20 or len(async_frames) >= 10
    }

def _update_request_counts(self, params: dict):
    """更新API和资源计数"""
    url = params.get("request", {}).get("url", "")
    origin, path = self._parse_origin_path(url)
    
    resource_type = params.get("type", "")
    if resource_type in ["XHR", "Fetch"]:
        self.api_count[(origin, path)] = self.api_count.get((origin, path), 0) + 1
    elif resource_type == "Script" and any(ext in path for ext in ['.js', '.json']):
        self.resource_count[(origin, path)] = self.resource_count.get((origin, path), 0) + 1
```

#### 3. 触发与确认（100KB 阈值，v1 简化）

Layer 1: requestWillBeSent（候选缓存）
- 资源类型过滤：XHR/Fetch 一律缓存裁剪后的 initiator 快照（进入 LRU）；
- Script 仅当存在 initiator.stack 时缓存（动态 import/createElement 场景）；
- 大上传检测：`postData > 100KB` → 记录 `large_upload` 原因用于后续。

Layer 2: loadingFinished（最终确认）
- 大下载检测：`encodedDataLength > 100KB` → 确认 `large_download`；
- 高频 API：`(origin, path)` 计数 > 50 → 确认 `high_frequency_api_{count}`；
- 重复资源：`(origin, path)` 计数 > 5 且单次 > 10KB → 确认 `repeated_resource_{count}`。

满足任一条件：
- 若存在候选缓存 → `detailedStack.enabled=true` 并输出 frames/asyncFrames；
- 若不存在候选缓存 → `detailedStack.enabled=false` 但仍输出 `reason` 与 `collectionTime`（避免空档）。

```python
def _should_cache_initiator(self, params: dict) -> str:
    """判断是否需要缓存initiator（requestWillBeSent阶段）"""
    resource_type = params.get("type", "")
    
    # 主要目标：XHR/Fetch（数据请求）
    if resource_type in ["XHR", "Fetch"]:
        request = params.get("request", {})
        # 大上传立即标记
        if len(request.get("postData", "")) > 102400:  # 100KB
            return "large_upload"
        # XHR/Fetch都预缓存（主要价值场景）
        return "xhr_fetch_candidate"
    
    # 补充目标：有JS栈的Script（动态加载场景）
    elif resource_type == "Script" and params.get("initiator", {}).get("stack"):
        return "script_with_stack"
    
    return None  # 其他类型不缓存

def _confirm_detailed_stack_needed(self, url: str, params: dict) -> str:
    """最终确认是否需要详细栈（loadingFinished阶段）"""
    # 大下载确认
    encoded_length = params.get("encodedDataLength", 0)
    if encoded_length > 102400:  # 100KB
        return "large_download"
    
    # 计数触发确认
    origin, path = self._parse_origin_path(url)  # 去除query参数
    
    # 高频API（>50次）
    api_count = self.api_count.get((origin, path), 0)
    if api_count > 50:
        return f"high_frequency_api_{api_count}"
        
    # 重复资源（>5次且单次>10KB） 
    resource_count = self.resource_count.get((origin, path), 0)
    if resource_count > 5 and encoded_length > 10240:  # 10KB
        return f"repeated_resource_{resource_count}"
    
    return None

def _parse_origin_path(self, url: str) -> tuple:
    """解析URL为(origin, path)，去除query干扰"""
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        origin = f"{parsed.scheme}://{parsed.netloc}"
        path = parsed.path or "/"
        return (origin, path)
    except:
        return (url, "/")  # 回退
```

#### 4. 数据结构设计（reason 统一命名）

**扩展现有network.jsonl格式**：
```json
{
  "type": "network_request_complete",
  "url": "https://api.example.com/export.json",
  "method": "POST",
  "status": 200,
  "encodedDataLength": 5242880,
  "initiator": {
    "type": "script",
    "source": {
      "function": "fetchData",
      "url": "https://app.js", 
      "line": 42
    }
  },
  "detailedStack": {
    "enabled": true,
    "reason": "high_frequency_api_3939",  // 或 "large_download" / "large_upload" / "repeated_resource_6"
    "frames": [
      {
        "functionName": "ComponentA.fetchData",
        "url": "https://app.js",
        "lineNumber": 42,
        "columnNumber": 15,
        "scriptId": "123"
      },
      {
        "functionName": "ComponentA.init", 
        "url": "https://app.js",
        "lineNumber": 28,
        "columnNumber": 8,
        "scriptId": "123"
      },
      {
        "functionName": "App.loadComponent",
        "url": "https://main.js",
        "lineNumber": 156,
        "columnNumber": 4,
        "scriptId": "124"
      }
    ],
    "asyncFrames": [
      {
        "functionName": "setTimeout callback",
        "url": "https://scheduler.js", 
        "lineNumber": 89,
        "columnNumber": 12,
        "scriptId": "125"
      }
    ],
    "truncated": false,
    "collectionTime": "2025-08-18T14:51:26.789Z"
  }
}
```

**字段说明**：
- `detailedStack`：新增字段，仅大请求包含
- `frames`：主调用栈，按调用顺序排列
- `asyncFrames`：异步调用栈，跨Promise/setTimeout等
- `truncated`：是否因深度限制被截断
- `collectionTime`：栈收集时间戳

### 架构图（两阶段：候选缓存 + 完成确认）

```
NetworkMonitor 增强架构（三层触发策略）

启动阶段：
└── start_monitoring() → _enable_debugger_globally() [全局一次性启用]

请求开始阶段（requestWillBeSent）：
├── 现有流程保持不变 → _format_initiator_simple() [简单模式]
└── 新增候选缓存 → _should_cache_initiator() [XHR/Fetch + Script]
    ├── 大上传检测 (postData > 100KB)
    ├── LRU缓存管理 (max 300)
    └── _cache_trimmed_initiator() [立即裁剪]

请求完成阶段（loadingFinished）：
├── 现有流程保持不变 → network.jsonl [基础数据]
└── 新增栈增强逻辑
    ├── _confirm_detailed_stack_needed() [最终确认]
    │   ├── 大下载检测 (encodedDataLength > 100KB)
    │   ├── 高频API检测 (>50次)
    │   └── 重复资源检测 (>5次且>10KB)
    ├── _format_detailed_stack() [解析缓存的快照栈]
    └── 附加detailedStack字段 → network.jsonl

内存控制：
├── 候选缓存：LRU淘汰 (300个上限)
├── 计数优化：(origin, path) 去query干扰
└── 栈裁剪：frames≤20, asyncFrames≤10
```

### 集成方式

**无需修改现有集成代码**：
- CLI路由：现有`comprehensive_data_callback`自动处理
- 数据写入：现有`DataManager.write_network_data()`自动兼容
- 文件格式：现有network.jsonl格式扩展，向后兼容

## Tests

### 测试驱动开发方案

#### 1. 单元测试

**核心测试类**：
```python
# tests/test_network_stack_enhancement.py

class TestNetworkStackEnhancement:
    
    @pytest.fixture 
    def enhanced_network_monitor(self):
        """创建启用栈增强的NetworkMonitor"""
        mock_connector = AsyncMock()
        event_queue = asyncio.Queue()
        monitor = NetworkMonitor(mock_connector, "test_session", event_queue)
        return monitor
    
    def test_should_cache_initiator_large_upload(self, enhanced_network_monitor):
        """测试大上传（>100KB）触发缓存"""
        params = {
            "type": "XHR",
            "requestId": "req123",
            "request": {
                "url": "https://api.com/upload",
                "postData": "x" * (102400 + 1)  # 100KB + 1
            }
        }
        result = enhanced_network_monitor._should_cache_initiator(params)
        assert result == "large_upload"
    
    def test_should_cache_initiator_xhr_fetch(self, enhanced_network_monitor):
        """测试XHR/Fetch请求触发缓存"""
        params = {"type": "XHR", "request": {"url": "https://api.com/data"}}
        assert enhanced_network_monitor._should_cache_initiator(params) == "xhr_fetch_candidate"
        
        params = {"type": "Fetch", "request": {"url": "https://api.com/data"}}
        assert enhanced_network_monitor._should_cache_initiator(params) == "xhr_fetch_candidate"
    
    def test_should_cache_initiator_script_with_stack(self, enhanced_network_monitor):
        """测试有JS栈的Script触发缓存"""
        params = {
            "type": "Script",
            "request": {"url": "https://cdn.com/dynamic.js"},
            "initiator": {"stack": {"callFrames": [{"functionName": "loadScript"}]}}
        }
        assert enhanced_network_monitor._should_cache_initiator(params) == "script_with_stack"
        
        # 无栈的Script不缓存
        params["initiator"] = {"type": "parser"}
        assert enhanced_network_monitor._should_cache_initiator(params) is None
    
    def test_confirm_detailed_stack_large_download(self, enhanced_network_monitor):
        """测试大下载确认"""
        params = {"encodedDataLength": 102401}  # 100KB + 1
        assert enhanced_network_monitor._confirm_detailed_stack_needed("https://api.com/export", params) == "large_download"
    
    def test_confirm_detailed_stack_high_frequency_api(self, enhanced_network_monitor):
        """测试高频API确认"""
        # 设置API计数
        enhanced_network_monitor.api_count[("https://api.com", "/v2/trades")] = 55
        params = {"encodedDataLength": 1000}
        result = enhanced_network_monitor._confirm_detailed_stack_needed("https://api.com/v2/trades", params)
        assert result == "high_frequency_api_55"
    
    def test_should_not_collect_for_normal_requests(self, enhanced_network_monitor):
        """测试正常请求不触发详细栈"""
        # 小文件（不构成大上传候选）
        params = {"type": "XHR", "request": {"url": "https://api.com/status", "postData": "small"}}
        assert enhanced_network_monitor._should_cache_initiator(params) == "xhr_fetch_candidate"
        # 完成阶段长度不达标 → 不触发
        assert enhanced_network_monitor._confirm_detailed_stack_needed("https://api.com/status", {"encodedDataLength": 1000}) is None
        
    def test_parse_call_frames_basic(self, enhanced_network_monitor):
        """测试基础调用帧解析"""
        call_frames = [
            {
                "functionName": "testFunction",
                "url": "https://test.js",
                "lineNumber": 42,
                "columnNumber": 8,
                "scriptId": "123"
            }
        ]
        result = enhanced_network_monitor._parse_call_frames(call_frames)
        assert len(result) == 1
        assert result[0]["functionName"] == "testFunction"
        assert result[0]["url"] == "https://test.js"
        assert result[0]["lineNumber"] == 42
        
    def test_traverse_async_stack_with_parent(self, enhanced_network_monitor):
        """测试异步栈遍历"""
        initiator = {
            "stack": {
                "callFrames": [{"functionName": "main", "url": "main.js", "lineNumber": 1}],
                "parent": {
                    "callFrames": [{"functionName": "async_parent", "url": "async.js", "lineNumber": 10}]
                }
            }
        }
        result = enhanced_network_monitor._format_detailed_stack(initiator)
        assert len(result["frames"]) == 1
        assert len(result["asyncFrames"]) == 1
        assert result["asyncFrames"][0]["functionName"] == "async_parent"
        
    def test_stack_depth_limitation(self, enhanced_network_monitor):
        """测试栈深度限制（主栈≤20，父链≤10）"""
        # 这里作为设计验证：实现中在 _trim_initiator_snapshot 内裁剪
        pass
        
    @pytest.mark.asyncio
    async def test_debugger_enable_graceful_failure(self, enhanced_network_monitor):
        """测试Debugger启用失败的优雅处理"""
        # Mock Debugger.enable失败
        enhanced_network_monitor.connector.call.side_effect = Exception("Debugger unavailable")
        
        await enhanced_network_monitor._enable_debugger_globally()
        
        # 应该优雅失败，不抛出异常
        assert enhanced_network_monitor.debugger_enabled is False
        
    def test_existing_functionality_unchanged(self, enhanced_network_monitor):
        """测试现有简单initiator功能完全不变"""
        initiator = {
            "type": "script",
            "stack": {
                "callFrames": [
                    {"functionName": "existing", "url": "existing.js", "lineNumber": 42}
                ]
            }
        }
        result = enhanced_network_monitor._format_initiator_simple(initiator)
        
        # 验证现有格式完全不变
        expected = {
            "type": "script",
            "source": {
                "function": "existing",
                "url": "existing.js",
                "line": 42
            }
        }
        assert result == expected

    def test_disabled_detailed_stack_when_no_candidate(self, enhanced_network_monitor):
        """触发条件满足但无候选缓存时，仍应输出 disabled 的详细栈（仅 reason）"""
        # 设计级示例：具体断言在集成测试中完成
        pass
```

#### 2. 集成测试

**端到端测试场景**：
```python
# tests/test_network_stack_integration.py

class TestNetworkStackIntegration:
    
    @pytest.mark.asyncio
    async def test_large_request_detailed_stack_collection(self):
        """测试大请求的完整栈收集流程"""
        # 设置环境：mock Chrome连接，大文件上传场景
        # 验证：detailedStack字段被正确添加到输出数据
        pass
        
    @pytest.mark.asyncio 
    async def test_small_request_performance_unchanged(self):
        """测试小请求性能完全不受影响"""
        # 验证：小请求处理时间与原版本一致
        # 验证：不会调用Debugger相关方法
        pass
        
    @pytest.mark.asyncio
    async def test_data_manager_integration(self):
        """测试与DataManager的集成"""
        # 验证：增强的network数据能正确写入network.jsonl
        # 验证：文件格式向后兼容
        pass
        
    @pytest.mark.asyncio
    async def test_real_browser_stack_collection(self):
        """真实浏览器环境的栈收集测试"""
        # 需要真实Chrome实例
        # 验证：能获取到真实的JavaScript调用栈
        pass
```

#### 3. 性能测试

**性能回归验证**：
```python
# tests/test_network_stack_performance.py

class TestNetworkStackPerformance:
    
    @pytest.mark.performance
    async def test_minimal_performance_impact(self):
        """验证性能影响最小（仅2.7%的URL需要栈）"""
        # 模拟真实场景：1000个请求，其中~27个触发栈收集
        # 验证：总处理时间增长 <5%
        # 验证：内存增长 <1MB（27个栈 * ~5KB）
        pass
        
    @pytest.mark.performance  
    async def test_high_frequency_api_overhead(self):
        """验证高频API的栈收集开销"""
        # 模拟场景：API被调用3939次（如实际数据）
        # 验证：仅第51次开始收集栈，之前50次无开销
        # 验证：栈收集总开销 <1秒
        pass
```

### 测试数据准备

**Mock数据示例**：
```python
MOCK_COMPLEX_STACK = {
    "type": "script",
    "stack": {
        "callFrames": [
            {
                "functionName": "ComponentA.fetchLargeData", 
                "scriptId": "123",
                "url": "https://app.js",
                "lineNumber": 42,
                "columnNumber": 15
            },
            {
                "functionName": "ComponentA.init",
                "scriptId": "123", 
                "url": "https://app.js",
                "lineNumber": 28,
                "columnNumber": 8
            }
        ],
        "parent": {
            "callFrames": [
                {
                    "functionName": "setTimeout",
                    "scriptId": "124",
                    "url": "https://scheduler.js", 
                    "lineNumber": 89,
                    "columnNumber": 12
                }
            ]
        }
    }
}
```

### 测试覆盖目标

- **单元测试覆盖率**：>95%（所有新增方法）
- **集成测试覆盖率**：主要数据流路径100%
- **边界测试**：栈深度限制、Debugger失败、异常数据格式
- **性能测试**：小请求零影响、大请求可接受开销
- **兼容性测试**：现有功能完全不变

免测内容：无，所有新增功能都需要完整测试覆盖。

## 设计优化改进

### 1. 栈处理精细度调整

**问题分析**：当前20帧主栈+10层异步栈的限制可能过于保守，trimmed snapshot的字符串截断可能丢失关键调试信息。

**改进方案**：
```python
def _trim_initiator_snapshot(self, raw_initiator: dict) -> dict:
    """裁剪initiator快照以控制内存（调整后的精细度）"""
    if not raw_initiator.get("stack"):
        return {"type": raw_initiator.get("type", "unknown")}
        
    trimmed = {"type": raw_initiator.get("type", "unknown")}
    stack = raw_initiator["stack"]
    
    # 调整：增加主栈帧数限制到30帧，保留更多调试信息
    if stack.get("callFrames"):
        def trim_frames(frames, limit):
            out = []
            for frame in frames[:limit]:
                out.append({
                    "functionName": str(frame.get("functionName", ""))[:150],  # 增加到150字符
                    "url": str(frame.get("url", ""))[:300],  # 增加到300字符
                    "lineNumber": int(frame.get("lineNumber", 0)),
                    "columnNumber": int(frame.get("columnNumber", 0)),
                    "scriptId": str(frame.get("scriptId", ""))[:50]
                })
            return out

        trimmed_stack = {"callFrames": trim_frames(stack.get("callFrames", []), 30)}  # 增加到30帧

        # 调整：异步父栈链增加到15层，保留更多异步上下文
        parent_src = stack
        parent_dst = trimmed_stack
        depth = 0
        while parent_src.get("parent") and depth < 15:  # 增加到15层
            parent_src = parent_src["parent"]
            node = {"callFrames": trim_frames(parent_src.get("callFrames", []), 30)}  # 每层也增加到30帧
            parent_dst["parent"] = node
            parent_dst = node
            depth += 1

        trimmed["stack"] = trimmed_stack
    
    return trimmed

def _format_detailed_stack(self, trimmed_initiator: dict) -> dict:
    """解析缓存的调用栈快照（调整截断判断）"""
    frames = []
    async_frames = []
    
    stack = trimmed_initiator.get("stack", {})
    
    # 解析主调用栈
    if stack.get("callFrames"):
        frames = stack["callFrames"]  # 已经在缓存时裁剪过
        
    # 解析异步调用栈（parent属性）
    current_stack = stack
    while current_stack.get("parent"):
        current_stack = current_stack["parent"]
        if current_stack.get("callFrames"):
            async_frames.extend(current_stack["callFrames"])
    
    return {
        "frames": frames,
        "asyncFrames": async_frames[:15],  # 调整二次保护限制
        "truncated": len(frames) >= 30 or len(async_frames) >= 15  # 调整截断判断
    }
```

### 2. 增强调试能力

**问题分析**：当前设计缺少详细的调试统计和运行时监控能力，故障排查困难。

**改进方案**：
```python
class NetworkMonitor:
    def __init__(self, ...):
        # 现有属性保持不变
        # ...
        
        # 新增调试统计属性
        self._recent_triggers = []  # 最近触发的详细栈收集记录
        self._debug_stats = {
            "total_candidates_cached": 0,
            "total_stacks_collected": 0,
            "debugger_enable_attempts": 0,
            "debugger_enable_failures": 0
        }
    
    def get_debug_stats(self) -> dict:
        """获取调试统计信息"""
        return {
            "candidates_cached": len(self.stack_candidates),
            "api_count_entries": len(self.api_count),
            "resource_count_entries": len(self.resource_count),
            "debugger_enabled": self.debugger_enabled,
            "recent_triggers": self._recent_triggers[-10:],  # 最近10次触发记录
            "lifetime_stats": self._debug_stats.copy()
        }
    
    def _record_trigger_event(self, reason: str, request_id: str, url: str, enabled: bool):
        """记录触发事件用于调试"""
        trigger_record = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "reason": reason,
            "requestId": request_id,
            "url": url[:100],
            "enabled": enabled
        }
        self._recent_triggers.append(trigger_record)
        
        # 保持最近50条记录
        if len(self._recent_triggers) > 50:
            self._recent_triggers.pop(0)
            
        # 更新生命周期统计
        if enabled:
            self._debug_stats["total_stacks_collected"] += 1

    async def _enable_debugger_globally(self):
        """全局一次性启用Debugger domain（增加调试统计）"""
        self._debug_stats["debugger_enable_attempts"] += 1
        try:
            await self.connector.call("Debugger.enable", session_id=self.session_id)
            await self.connector.call("Debugger.setAsyncCallStackDepth", 
                                    {"maxDepth": 15}, session_id=self.session_id)  # 调整深度到15
            self.debugger_enabled = True
            logger.info(f"Debugger enabled for detailed stack collection (session: {self.session_id})")
        except Exception as e:
            self._debug_stats["debugger_enable_failures"] += 1
            logger.warning(f"Failed to enable debugger for session {self.session_id}: {e}")
            # 优雅降级，不影响基础网络监控

    def _cache_trimmed_initiator(self, request_id: str, raw_initiator: dict, url: str, resource_type: str):
        """缓存裁剪后的initiator快照（增加调试统计）"""
        # LRU淘汰策略
        if len(self.stack_candidates) >= self.max_candidates:
            oldest_id = min(self.stack_candidates, key=lambda k: self.stack_candidates[k]["cached_at"])
            self.stack_candidates.pop(oldest_id, None)
        
        # 立即裁剪：调整后的精细度
        trimmed = self._trim_initiator_snapshot(raw_initiator)
        self.stack_candidates[request_id] = {
            "snapshot": trimmed,
            "cached_at": datetime.now(timezone.utc).timestamp(),
            "url": url,
            "resource_type": resource_type
        }
        
        # 更新调试统计
        self._debug_stats["total_candidates_cached"] += 1

    async def _on_request_finished(self, params: dict) -> None:
        # 现有逻辑保持不变...
        
        # 新增：最终确认并附加detailedStack（增加调试记录）
        final_reason = self._confirm_detailed_stack_needed(url=request_data.get("url", ""), params=params)
        if final_reason:
            candidate = self.stack_candidates.get(request_id)
            if candidate:
                detailed_stack = self._format_detailed_stack(candidate["snapshot"])
                request_data["detailedStack"] = {
                    "enabled": True,
                    "reason": final_reason,
                    "collectionTime": datetime.now(timezone.utc).isoformat(),
                    **detailed_stack
                }
                # 记录成功触发事件
                self._record_trigger_event(final_reason, request_id, request_data.get("url", ""), True)
            else:
                request_data["detailedStack"] = {
                    "enabled": False,
                    "reason": final_reason,
                    "collectionTime": datetime.now(timezone.utc).isoformat()
                }
                # 记录失败触发事件（有原因但无候选）
                self._record_trigger_event(final_reason, request_id, request_data.get("url", ""), False)
        
        # 清理候选缓存
        self.stack_candidates.pop(request_id, None)
        
        # 现有逻辑继续（完全不变）
        self.event_queue.put_nowait(("network_request_complete", request_data))
```

**调试接口集成**：
```python
# CLI增强：添加调试统计命令
async def show_network_debug_stats():
    """显示网络监控调试统计"""
    # 通过现有架构获取NetworkMonitor实例
    if hasattr(monitor, 'get_debug_stats'):
        stats = monitor.get_debug_stats()
        print(json.dumps(stats, indent=2))
```

### 优化效果评估

**栈精细度调整**：
- 主栈帧数：20 → 30帧（+50%调试信息）
- 异步栈层数：10 → 15层（+50%异步上下文）
- 字符串长度：functionName 100→150，url 200→300（减少关键信息丢失）
- 内存影响：单个栈约5KB → 7.5KB（可接受）

**调试能力增强**：
- 实时统计：候选缓存数、计数器条目数、Debugger状态
- 历史追踪：最近50次触发记录，包含成功/失败状态
- 生命周期统计：总缓存数、总收集数、Debugger启用次数
- CLI集成：便于运行时诊断和性能分析
