# Design 1-7 - 按网站分组的数据管理器

## Requirements

### 核心目标
基于已完成的DataManager(1-5)和多指标收集器(1-6)，增加高级数据组织和分析功能，为用户提供按网站维度的性能洞察。

### 数据组织需求
- **智能域名分组**：使用eTLD+1规则将子域名归类到主站（如github.com包含gist.github.com）
- **会话间数据聚合**：跨多个监控会话查询和对比网站性能
- **数据完整性检查**：验证JSONL文件格式和数据连续性
- **存储空间管理**：提供数据清理和归档功能

### 分析功能需求
- **性能基线建立**：为每个网站建立内存使用、响应时间等基线指标
- **趋势分析**：检测网站性能随时间的变化趋势（恶化/改善）
- **异常检测**：识别明显偏离基线的性能异常
- **跨站对比**：同类网站的性能对比分析

### 查询接口需求
- **按网站查询**：检索特定网站的完整历史数据
- **按时间范围查询**：获取指定时间段的性能数据
- **按指标类型查询**：分别查询内存、网络、Console等不同类型数据
- **汇总统计查询**：P95响应时间、平均内存使用等统计指标

### 数据导出需求
- **结构化报告生成**：生成包含图表和分析结论的HTML/PDF报告
- **原始数据导出**：支持CSV、JSON等格式的数据导出
- **开发团队协作**：生成适合开发团队分析的数据包

## Solution

### 核心设计原则
- **最小扩展**：基于现有DataManager架构，不破坏已有功能
- **实用优先**：专注核心查询和分组需求，避免过度设计
- **TDD驱动**：先写测试，确保每个功能可验证
- **向下兼容**：完全不影响现有1-5数据写入功能

### 1. SiteDataManager类（按现有路径约定读取）
```python
# browserfairy/data/site_manager.py (新文件)
class SiteDataManager:
    """网站数据查询管理器 - 按现有结构读取session_*/{hostname}/*.jsonl"""
    
    def __init__(self, data_dir: Optional[Path] = None):
        self.data_dir = data_dir or get_data_directory()
        
    def get_all_sessions(self) -> List[str]:
        """获取所有会话ID列表"""
        sessions = []
        try:
            for item in self.data_dir.iterdir():
                if item.is_dir() and item.name.startswith('session_'):
                    sessions.append(item.name)
        except Exception:
            pass  # 目录不存在等情况静默跳过
        return sorted(sessions)
        
    def get_sites_for_session(self, session_id: str) -> List[str]:
        """获取指定会话的所有网站"""
        session_path = self.data_dir / session_id
        if not session_path.exists():
            return []
            
        sites = []
        try:
            for item in session_path.iterdir():
                if item.is_dir() and not item.name.startswith('.'):
                    sites.append(item.name)
        except Exception:
            pass
        return sorted(sites)
        
    def get_site_data_generator(self, session_id: str, hostname: str, 
                               data_type: str = "memory"):
        """获取网站数据生成器 - 避免一次性加载"""
        # 按约定路径读取：session_*/{hostname}/{memory|network|console|correlations}.jsonl
        file_path = self.data_dir / session_id / hostname / f"{data_type}.jsonl"
        return read_jsonl_data(file_path)
        
    def get_site_memory_stats(self, session_id: str, hostname: str) -> Dict:
        """获取网站内存统计（使用生成器）"""
        data_gen = self.get_site_data_generator(session_id, hostname, "memory")
        return calculate_memory_stats(data_gen)
```

### 2. 极简域名分组算法
```python
def normalize_hostname(hostname: str) -> str:
    """极简hostname规范化 - 只处理www和m前缀"""
    if not hostname:
        return "unknown"
    
    hostname = hostname.lower()
    
    # 只移除www和m前缀，不处理api等（避免误合并不同服务）
    if hostname.startswith("www."):
        hostname = hostname[4:]
    elif hostname.startswith("m."):
        hostname = hostname[2:]
    
    return hostname
    
def group_hostnames(hostnames: List[str]) -> Dict[str, List[str]]:
    """简单分组 - 仅用于展示汇总，不改动文件结构"""
    groups = {}
    for hostname in hostnames:
        normalized = normalize_hostname(hostname)
        if normalized not in groups:
            groups[normalized] = []
        groups[normalized].append(hostname)
    return groups
```

### 3. P95统计分析（简单排序法）
```python
def calculate_memory_stats(data_generator) -> Dict:
    """计算内存统计 - 使用生成器避免一次性加载"""
    values = []
    count = 0
    
    for record in data_generator:
        try:
            memory = record.get("memory", {})
            js_heap = memory.get("jsHeap", {})
            used = js_heap.get("used", 0)
            if used > 0:
                values.append(used)
                count += 1
                
            # 软上限：避免内存冲击，超过100k条就截断
            if count >= 100000:
                break
        except Exception:
            # 忽略坏行，继续处理
            continue
    
    if not values:
        return {"count": 0}
    
    # 简单排序法计算P95
    values.sort()
    n = len(values)
    p95_index = int(0.95 * (n - 1))
    
    return {
        "count": n,
        "min": values[0],
        "max": values[-1],
        "avg": sum(values) // n,
        "p95": values[p95_index]
    }

def read_jsonl_data(file_path: Path):
    """逐行读取JSONL - 生成器模式避免一次性加载"""
    if not file_path.exists():
        return  # 文件缺失直接跳过，不报错
        
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    yield json.loads(line)
                except json.JSONDecodeError:
                    # 忽略坏行，继续处理下一行
                    continue
    except Exception:
        # 文件读取失败，静默跳过
        return
```

### 4. CLI集成（简单查询命令）
```python
# cli.py扩展
async def analyze_site_data(hostname: str = None):
    """分析网站数据命令"""
    # --analyze-sites [hostname] 
    # 显示网站监控概览或特定网站详情
```

### 技术实现要点（按用户必须项要求）

**文件遍历与读取**：
- **只读JSONL**：用逐行迭代读取（yield生成器）+ try/except忽略坏行，避免一次性加载
- **路径约定**：按现有结构读取`session_*/{hostname}/{memory|network|console|correlations}.jsonl`
- **文件缺失处理**：直接跳过，不报错

**P95实现**：
- **简单排序法**：收集目标值（如内存used），排序后取`int(0.95*(n-1))`索引
- **软上限保护**：最多100k条再排序，避免内存冲击；超过上限就截断
- **不用复杂算法**：不引入近似算法，保持实现简单

**Host规范化**：
- **极简规则**：只去掉前缀`www.`和`m.`，统一小写
- **不误合并**：不默认移除`api.`等，避免把不同服务合并
- **仅用于展示**：分组仅用于展示与汇总，不改动目录结构与文件名

### 与现有系统集成
- **不修改DataManager**：保持1-5数据写入功能完全不变
- **不修改DataWriter**：继续按现有格式写入JSONL文件
- **不修改目录结构**：使用现有session_xxx/hostname/格式
- **独立CLI命令**：新增`--analyze-sites`，不影响现有命令

## Tests

### TDD测试策略（优先级驱动）

**阶段1：核心读取功能测试**
```python
class TestSiteDataManager:
    @pytest.fixture
    def sample_data_structure(self, tmp_path):
        """创建标准的测试数据目录结构"""
        # 模拟现有DataWriter生成的目录结构
        
    def test_get_all_sessions(self, sample_data_structure):
        """测试会话列表获取正确性"""
        
    def test_get_sites_for_session(self, sample_data_structure):
        """测试单会话网站列表获取"""
        
    def test_get_site_data_memory(self, sample_data_structure):
        """测试内存数据读取和解析"""
        
    def test_get_site_data_empty_file(self, sample_data_structure):
        """测试空文件和不存在文件的处理"""
```

**阶段2：域名分组测试（极简规则）**
```python
def test_normalize_hostname():
    """测试极简域名规范化 - 只处理www和m前缀"""
    assert normalize_hostname("www.github.com") == "github.com"
    assert normalize_hostname("m.facebook.com") == "facebook.com"
    assert normalize_hostname("API.GITHUB.COM") == "api.github.com"  # 不移除api
    assert normalize_hostname("gist.github.com") == "gist.github.com"  # 不移除gist
    
def test_group_hostnames():
    """测试分组功能 - 仅用于展示，不改目录结构"""
    hostnames = ["github.com", "www.github.com", "api.github.com"]
    groups = group_hostnames(hostnames)
    # github.com组包含github.com和www.github.com
    # api.github.com独立分组（避免误合并）
    assert len(groups["github.com"]) == 2
    assert "api.github.com" in groups
```

**阶段3：P95统计测试（简单排序法）**
```python  
def test_calculate_memory_stats():
    """测试P95计算 - 简单排序法"""
    def sample_generator():
        yield {"memory": {"jsHeap": {"used": 1000000}}}
        yield {"memory": {"jsHeap": {"used": 2000000}}}
        yield {"memory": {"jsHeap": {"used": 3000000}}}
        yield {"memory": {"jsHeap": {"used": 4000000}}}
        yield {"memory": {"jsHeap": {"used": 5000000}}}
        yield {"invalid": "data"}  # 测试坏数据忽略
        
    stats = calculate_memory_stats(sample_generator())
    assert stats["count"] == 5
    assert stats["min"] == 1000000
    assert stats["max"] == 5000000
    assert stats["avg"] == 3000000
    # P95: int(0.95 * (5-1)) = 3, 即第4个值
    assert stats["p95"] == 4000000

def test_read_jsonl_data_generator():
    """测试JSONL读取生成器 - 忽略坏行，文件缺失跳过"""
    # 测试生成器能正确处理坏JSON行
    # 测试文件不存在时静默跳过
```

**阶段4：CLI集成测试**
```python
def test_analyze_sites_command():
    """测试CLI命令的参数解析和输出"""
    # 验证--analyze-sites命令正常工作
```

### 验收测试标准
**功能验收**：
- ✅ 能正确读取现有DataWriter生成的所有数据文件
- ✅ 域名分组算法处理常见子域名场景
- ✅ 统计计算结果准确（与手工计算一致）
- ✅ CLI命令输出友好可读

**兼容性验收**：
- ✅ 不破坏现有DataManager的任何功能
- ✅ 与现有CLI命令无冲突
- ✅ 处理各种数据文件异常情况不崩溃

**性能验收**：
- ✅ 查询大型会话数据(<10秒响应)
- ✅ 内存占用合理(<100MB for typical data)