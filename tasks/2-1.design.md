# Design 2-1 - 小幅事件和I/O优化

## Requirements

### 简单修复目标
**解决两个小的技术问题**：

1. **TabMonitor事件卸载修复**（3行代码）
   - **问题**：`stop_monitoring()`调用`off_event`未传handler参数
   - **风险**：可能误删其他组件的同类事件处理器
   - **位置**：tabs.py第88-90行

2. **DataWriter性能小优化**（简单延迟同步）
   - **问题**：每次写入都立即fsync，高频场景性能不佳
   - **影响**：comprehensive模式下I/O延迟较高
   - **位置**：writer.py第51-52行

### 最小影响原则
- 保持现有API完全不变
- 不修改架构设计
- 不引入复杂的线程安全机制
- 默认行为保持一致，可选启用优化

## Solution

### 简单修复方案

**1. TabMonitor事件卸载修复**（1分钟修复）

```python
# browserfairy/monitors/tabs.py - 第88-90行修复
# 修复前：
self.connector.off_event("Target.targetCreated")
self.connector.off_event("Target.targetDestroyed") 
self.connector.off_event("Target.targetInfoChanged")

# 修复后：
self.connector.off_event("Target.targetCreated", self._on_target_created)
self.connector.off_event("Target.targetDestroyed", self._on_target_destroyed)  
self.connector.off_event("Target.targetInfoChanged", self._on_target_info_changed)
```

**2. DataWriter性能小优化**（修正线程安全问题）

```python
# browserfairy/data/writer.py - 线程安全修正方案
class DataWriter:
    def __init__(self, session_dir: Path, enable_delayed_sync: bool = False):
        # 现有代码保持不变 
        self.enable_delayed_sync = enable_delayed_sync
        self._pending_sync_files = set()  # 待同步文件集合（仅在事件循环线程访问）
        
    def _sync_write_jsonl(self, file_path: Path, json_line: str) -> None:
        """文件写入 - 不操作共享集合（线程安全）"""
        with open(file_path, 'a', encoding='utf-8') as f:
            f.write(json_line)
            f.flush()
            
            if not self.enable_delayed_sync:
                # 默认模式：立即同步（保持原行为）
                os.fsync(f.fileno())
            # 延迟模式：什么都不做，让调用方处理
                
    async def append_jsonl(self, file_path: str, data: Dict[str, Any]) -> None:
        """修正版本 - 在事件循环线程中安全操作集合"""
        full_path = self.session_dir / file_path
        full_path.parent.mkdir(parents=True, exist_ok=True)
        
        async with self.file_locks[file_path]:
            # 轮转检查（修正：轮转前无条件fsync）
            await self._rotate_if_needed_safe(full_path)
            
            # 格式化JSON行
            json_line = json.dumps(data, ensure_ascii=False, separators=(',', ':')) + '\n'
            
            # 同步写入（在单独线程中执行）
            await asyncio.to_thread(self._sync_write_jsonl, full_path, json_line)
            
            # ✅ 修正：写入完成后，在事件循环线程中安全操作集合
            if self.enable_delayed_sync:
                self._pending_sync_files.add(full_path)
                
    async def _rotate_if_needed_safe(self, file_path: Path) -> None:
        """修正版本 - 轮转前无条件fsync避免数据丢失"""
        try:
            if not file_path.exists():
                return
                
            # 检查文件大小
            stat_result = await asyncio.to_thread(file_path.stat)
            if stat_result.st_size < self.MAX_FILE_SIZE:
                return
            
            # ✅ 修正：轮转前无条件fsync当前文件（与延迟设置无关）
            def _force_sync_before_rotation():
                try:
                    with open(file_path, 'r+b') as f:
                        os.fsync(f.fileno())
                except Exception as e:
                    logger.warning(f"Pre-rotation sync failed for {file_path}: {e}")
                    
            await asyncio.to_thread(_force_sync_before_rotation)
            
            # 从待同步集合中移除（即将被改名）
            self._pending_sync_files.discard(file_path)
            
            # 执行轮转（使用现有函数名）
            await asyncio.to_thread(self._sync_rotate_files, file_path)
            
        except Exception as e:
            logger.warning(f"File rotation failed for {file_path}: {e}")
    
    async def force_sync_pending(self) -> None:
        """强制同步所有待同步文件（会话结束时调用）"""
        if not self._pending_sync_files:
            return
            
        files_to_sync = self._pending_sync_files.copy()
        self._pending_sync_files.clear()
        
        def _batch_sync(file_paths):
            for file_path in file_paths:
                try:
                    with open(file_path, 'r+b') as f:
                        os.fsync(f.fileno())
                except Exception as e:
                    logger.warning(f"Force sync failed for {file_path}: {e}")
        
        await asyncio.to_thread(_batch_sync, files_to_sync)
```

**3. DataManager集成修正**（确保延迟模式安全）

```python
# browserfairy/data/manager.py - DataManager.stop()单行修正
async def stop(self) -> None:
    """停止数据管理和清理"""
    self.running = False
    
    # ✅ 修正：延迟模式下确保数据落盘（单行改动，不影响默认行为）
    if hasattr(self, 'data_writer') and self.data_writer:
        await self.data_writer.force_sync_pending()
        
    await self.storage_monitor.stop()
```

**关键修正总结**：
1. **线程安全**：`_pending_sync_files`只在事件循环线程中操作，避免并发修改set
2. **轮转安全**：轮转前无条件fsync，避免重命名文件的数据丢失窗口  
3. **函数名一致**：使用现有的`_sync_rotate_files`，避免重命名导致测试失效
4. **集成点明确**：在`DataManager.stop()`中调用`force_sync_pending()`确保数据安全
5. **最小影响**：默认行为完全不变，所有修改都向下兼容

## Tests

### 简单验证测试

**TabMonitor事件修复测试**：
```python
async def test_tab_monitor_off_event_with_handler():
    """验证修复后TabMonitor正确传递handler参数"""
    connector = ChromeConnector()
    tab_monitor = TabMonitor(connector)
    
    # 启动监控（会注册handler）
    await tab_monitor.start_monitoring()
    
    # 停止监控（应该传递handler参数）
    await tab_monitor.stop_monitoring()
    
    # 验证off_event被正确调用（可通过mock验证）
    assert True  # 具体实现根据测试框架确定
```

**DataWriter延迟同步测试**：
```python
async def test_delayed_sync_option():
    """验证延迟同步功能可选启用"""
    session_dir = Path("test_delayed_sync")
    
    # 测试默认行为（立即同步）
    writer_immediate = DataWriter(session_dir, enable_delayed_sync=False)
    await writer_immediate.append_jsonl("test.jsonl", {"test": "data"})
    
    # 测试延迟同步模式
    writer_delayed = DataWriter(session_dir, enable_delayed_sync=True)
    await writer_delayed.append_jsonl("test_delayed.jsonl", {"test": "data"})
    await writer_delayed.force_sync_pending()  # 手动同步
    
    # 验证两种模式文件都正确创建
    assert (session_dir / "test.jsonl").exists()
    assert (session_dir / "test_delayed.jsonl").exists()
```

### 验收标准

**修复完成标准**：
- TabMonitor的3个off_event调用都传递handler参数
- DataWriter添加enable_delayed_sync选项和force_sync_pending方法
- 所有现有测试继续通过
- 默认行为完全不变

**预期效果**：
- 解决事件处理器误删风险
- 为高频写入场景提供性能优化选项
- 保持向下兼容性